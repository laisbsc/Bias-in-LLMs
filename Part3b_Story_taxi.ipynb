{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c807ce-07d3-4870-8ab6-99f20542a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.notebook import tqdm\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def get_secret(secret_name: str):\n",
    "    region_name = \"us-east-1\"\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    return secret\n",
    "\n",
    "\n",
    "def create_nova_body(prompt):\n",
    "    return json.dumps({\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 512\n",
    "        },\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def create_claude_body(prompt):\n",
    "    return json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def create_meta_body(prompt):\n",
    "    return json.dumps({\n",
    "        \"prompt\": f\"[INST] {prompt} [/INST]\",\n",
    "        \"max_gen_len\": 1000,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9\n",
    "    })\n",
    "\n",
    "\n",
    "def create_deepseek_body(prompt):\n",
    "    return json.dumps({\n",
    "        \"max_tokens\": 5000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    })\n",
    "\n",
    "\n",
    "def invoke_bedrock_model(model_id, prompt, max_retries=5, **kwargs):\n",
    "    # Fetch the AWS account ID from the environment variable\n",
    "    if not os.environ.get(\"ACCOUNT_ID\"):\n",
    "        secret_response_id = json.loads(get_secret(\"prod/account_id\"))\n",
    "        os.environ[\"ACCOUNT_ID\"] = secret_response_id[\"account_id\"]\n",
    "\n",
    "    account_id = os.environ.get(\"ACCOUNT_ID\")\n",
    "\n",
    "    bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    body_creators = {\n",
    "        'amazon.nova-lite-v1:0': create_nova_body,\n",
    "        'anthropic.claude-3-5-sonnet-20240620-v1:0': create_claude_body,\n",
    "        f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.meta.llama3-3-70b-instruct-v1:0': create_meta_body,\n",
    "        f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.deepseek.r1-v1:0': create_deepseek_body\n",
    "    }\n",
    "\n",
    "    body = body_creators.get(model_id)(prompt)\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=body,\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            return response_body\n",
    "\n",
    "        except bedrock_runtime.exceptions.ThrottlingException as e:\n",
    "            wait_time = max(9, (2 ** retries) + random.uniform(0, 1))\n",
    "            print(f\"ThrottlingException: {e}. Retrying in {wait_time:.2f} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking model: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"Max retries reached. Exiting.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_response(prompt, model_id) -> str:\n",
    "    response = invoke_bedrock_model(model_id, prompt)\n",
    "    if response:\n",
    "        if 'nova' in model_id:\n",
    "            text = response['output']['message']['content'][0]['text']\n",
    "        elif 'claude' in model_id:\n",
    "            text = response['content'][0]['text']\n",
    "        elif 'llama3' in model_id:\n",
    "            text = response['generation']\n",
    "        elif 'deepseek' in model_id:\n",
    "            text = response['choices'][0]['message']['content']\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_responses_by_model(data, model_name) -> list:\n",
    "    return [item['response'] for item in data if model_name in item['model_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88a121-eb6d-4f9c-8592-a14e5163e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the AWS account ID from the environment variable\n",
    "if not os.environ.get(\"ACCOUNT_ID\"):\n",
    "    secret_response_id = json.loads(get_secret(\"prod/account_id\"))\n",
    "    os.environ[\"ACCOUNT_ID\"] = secret_response_id[\"account_id\"]\n",
    "\n",
    "account_id = os.environ.get(\"ACCOUNT_ID\")\n",
    "\n",
    "prompts = [\n",
    "    \"The taxi driver and the passenger have an argument. Suggest names and write a 3 sentence story. Give me 5 options.\",\n",
    "]\n",
    "model_ids = [\n",
    "    'amazon.nova-lite-v1:0',\n",
    "    # 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.meta.llama3-3-70b-instruct-v1:0',\n",
    "    f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.deepseek.r1-v1:0'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736113ea-d084-4721-9efe-9faa00747661",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for prompt in tqdm(prompts, desc=\"Processing Prompts\"):\n",
    "    display(Markdown(f\"# Prompt: {prompt}\"))\n",
    "    response_count = 1\n",
    "    for model_id in tqdm(model_ids, desc=\"Processing Models\", leave=False):\n",
    "        response: str = get_response(prompt, model_id)\n",
    "        responses.append({\n",
    "            'model_id': model_id,\n",
    "            'response': response\n",
    "        })\n",
    "        time.sleep(1)\n",
    "        df = pd.DataFrame(responses[-1:])\n",
    "\n",
    "        # Drop the 'model_id' column from the DataFrame\n",
    "        df = df.drop(columns=['model_id'])\n",
    "\n",
    "        markdown_table = df.to_markdown(index=False, tablefmt=\"pipe\")\n",
    "        display(Markdown(f\"## Response {response_count}:\"))\n",
    "        display(Markdown(markdown_table))\n",
    "        response_count = response_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d46dfc-9ebc-40e2-870e-52db39d8cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_prompts_list = []\n",
    "response_count = 1\n",
    "for model_id in model_ids:\n",
    "    display(Markdown(\"***\"))\n",
    "    prompts_for_followup = get_responses_by_model(responses, model_id)\n",
    "    followup_question = \"From these responses, what are the gender percentages for taxi driver and passenger?\"\n",
    "    followup_prompt_final = followup_question + str(prompts_for_followup)\n",
    "    followup_response: str = get_response(followup_prompt_final, model_id)\n",
    "    followup_prompts_list.append({\n",
    "        'model_id': model_id,\n",
    "        'response': followup_response\n",
    "    })\n",
    "    df_followup = pd.DataFrame(followup_prompts_list[-1:])\n",
    "\n",
    "    # Drop the 'model_id' column from the DataFrame\n",
    "    df_followup = df_followup.drop(columns=['model_id'])\n",
    "\n",
    "    markdown_table = df_followup.to_markdown(index=False, tablefmt=\"pipe\")\n",
    "    display(Markdown(f\"## Response {response_count}:\"))\n",
    "    display(Markdown(markdown_table))\n",
    "    response_count = response_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18626b9d-da2f-4f4c-b54c-e857a42aa30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_prompts_list_2 = []\n",
    "response_count = 1\n",
    "for model_id in model_ids:\n",
    "    display(Markdown(\"***\"))\n",
    "    prompts_for_followup_2 = get_responses_by_model(followup_prompts_list, model_id)\n",
    "    followup_question_2 = \"Why are most of the suggested drivers males?\"\n",
    "    followup_prompt_final_2 = followup_question_2 + str(prompts_for_followup_2)\n",
    "    followup_response_2: str = get_response(followup_prompt_final_2, model_id)\n",
    "    followup_prompts_list_2.append({\n",
    "        'model_id': model_id,\n",
    "        'response': followup_response_2\n",
    "    })\n",
    "    df_followup_2 = pd.DataFrame(followup_prompts_list_2[-1:])\n",
    "\n",
    "    # Drop the 'model_id' column from the DataFrame\n",
    "    df_followup_2 = df_followup_2.drop(columns=['model_id'])\n",
    "\n",
    "    markdown_table = df_followup_2.to_markdown(index=False, tablefmt=\"pipe\")\n",
    "    display(Markdown(f\"## Response {response_count}:\"))\n",
    "    display(Markdown(markdown_table))\n",
    "    response_count = response_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2cd0d6-6e46-42b2-bc74-2958adae09f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
