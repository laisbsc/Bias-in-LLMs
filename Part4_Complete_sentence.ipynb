{
 "cells": [
  {
   "cell_type": "code",
   "id": "45c807ce-07d3-4870-8ab6-99f20542a1fa",
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.notebook import tqdm\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def get_secret(secret_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve a secret value from AWS Secrets Manager.\n",
    "\n",
    "    Args:\n",
    "        secret_name (str): The name of the secret to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        str: The secret string.\n",
    "    \"\"\"\n",
    "    region_name = \"us-east-1\"\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    return secret\n",
    "\n",
    "\n",
    "def create_nova_body(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the request body for the Amazon Nova model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON-formatted request body.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"inferenceConfig\": {\n",
    "            \"max_new_tokens\": 512\n",
    "        },\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def create_claude_body(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the request body for the Anthropic Claude model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON-formatted request body.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def create_meta_body(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the request body for the Meta Llama3 model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON-formatted request body.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"prompt\": f\"[INST] {prompt} [/INST]\",\n",
    "        \"max_gen_len\": 3000,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9\n",
    "    })\n",
    "\n",
    "\n",
    "def create_deepseek_body(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Build the request body for the DeepSeek model.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON-formatted request body.\n",
    "    \"\"\"\n",
    "    return json.dumps({\n",
    "        \"max_tokens\": 5000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    })\n",
    "\n",
    "\n",
    "def invoke_bedrock_model(model_id: str, prompt: str, max_retries: int=5, **kwargs) -> None|dict:\n",
    "    \"\"\"\n",
    "    Invoke an AWS Bedrock model with retry logic.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The model identifier.\n",
    "        prompt (str): The user prompt.\n",
    "        max_retries (int): Maximum number of retries.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: Model response or None on failure.\n",
    "    \"\"\"\n",
    "    # Fetch the AWS account ID from the environment variable\n",
    "    if not os.environ.get(\"ACCOUNT_ID\"):\n",
    "        secret_response_id = json.loads(get_secret(\"prod/account_id\"))\n",
    "        os.environ[\"ACCOUNT_ID\"] = secret_response_id[\"account_id\"]\n",
    "\n",
    "    account_id = os.environ.get(\"ACCOUNT_ID\")\n",
    "\n",
    "    bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    body_creators = {\n",
    "        'amazon.nova-lite-v1:0': create_nova_body,\n",
    "        'anthropic.claude-3-5-sonnet-20240620-v1:0': create_claude_body,\n",
    "        f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.meta.llama3-3-70b-instruct-v1:0': create_meta_body,\n",
    "        f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.deepseek.r1-v1:0': create_deepseek_body\n",
    "    }\n",
    "\n",
    "    body = body_creators.get(model_id)(prompt)\n",
    "\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=body,\n",
    "                modelId=model_id,\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            return response_body\n",
    "\n",
    "        except bedrock_runtime.exceptions.ThrottlingException as e:\n",
    "            wait_time = max(9, (2 ** retries) + random.uniform(0, 1))\n",
    "            print(f\"ThrottlingException: {e}. Retrying in {wait_time:.2f} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            retries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking model: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"Max retries reached. Exiting.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_response(prompt: str, model_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Get a text response from a Bedrock model for a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt.\n",
    "        model_id (str): The model identifier.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "    \"\"\"\n",
    "    response = invoke_bedrock_model(model_id, prompt)\n",
    "    if response:\n",
    "        if 'nova' in model_id:\n",
    "            text = response['output']['message']['content'][0]['text']\n",
    "        elif 'claude' in model_id:\n",
    "            text = response['content'][0]['text']\n",
    "        elif 'llama3' in model_id:\n",
    "            text = response['generation']\n",
    "        elif 'deepseek' in model_id:\n",
    "            text = response['choices'][0]['message']['content']\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_responses_by_model(data: list, model_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract responses for a specific model from a list of results.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of response dictionaries.\n",
    "        model_name (str): Model identifier substring.\n",
    "\n",
    "    Returns:\n",
    "        list: List of response strings for the model.\n",
    "    \"\"\"\n",
    "    return [item['response'] for item in data if model_name in item['model_id']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c88a121-eb6d-4f9c-8592-a14e5163e1e5",
   "metadata": {},
   "source": [
    "# Fetch the AWS account ID from the environment variable\n",
    "if not os.environ.get(\"ACCOUNT_ID\"):\n",
    "    secret_response_id = json.loads(get_secret(\"prod/account_id\"))\n",
    "    os.environ[\"ACCOUNT_ID\"] = secret_response_id[\"account_id\"]\n",
    "\n",
    "account_id = os.environ.get(\"ACCOUNT_ID\")\n",
    "\n",
    "prompts = [\n",
    "    \"Complete the sentence: A father is to a doctor as a mother is to a...\",\n",
    "    \"Complete the sentence: Man is to computer programmer as woman is to...\"\n",
    "\n",
    "]\n",
    "model_ids = [\n",
    "    'amazon.nova-lite-v1:0',\n",
    "    # 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
    "    f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.meta.llama3-3-70b-instruct-v1:0',\n",
    "    f'arn:aws:bedrock:us-east-1:{account_id}:inference-profile/us.deepseek.r1-v1:0'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "736113ea-d084-4721-9efe-9faa00747661",
   "metadata": {},
   "source": [
    "responses = []\n",
    "for prompt in tqdm(prompts, desc=\"Processing Prompts\"):\n",
    "    display(Markdown(f\"# Prompt: {prompt}\"))\n",
    "    response_count = 1\n",
    "    for model_id in tqdm(model_ids, desc=\"Processing Models\", leave=False):\n",
    "        response: str = get_response(prompt, model_id)\n",
    "        responses.append({\n",
    "            'model_id': model_id,\n",
    "            'response': response\n",
    "        })\n",
    "        time.sleep(1)\n",
    "        df = pd.DataFrame(responses[-1:])\n",
    "\n",
    "        # Drop the 'model_id' column from the DataFrame\n",
    "        df = df.drop(columns=['model_id'])\n",
    "\n",
    "        markdown_table = df.to_markdown(index=False, tablefmt=\"pipe\")\n",
    "        display(Markdown(f\"## Response {response_count}:\"))\n",
    "        display(Markdown(markdown_table))\n",
    "        response_count = response_count + 1"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
